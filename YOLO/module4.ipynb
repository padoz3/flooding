{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import subprocess\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from scipy.stats import hmean\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 아래부터 추가해야 하는 라이브러리\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"UD\" # 불러올 이미지들의 디렉토리 명\n",
    "road_real_distance = 300 # 도로의 폭은 300cm 로 지정 / 만일 다른 기준객체를 참고하려면 해당 기준객체의 실 cm 값으로 변경\n",
    "\n",
    "s = 4 # 침수 전 이미지 프레임의 순서 \n",
    "f = 3 # 침수 후 이미지 프레임의 순서\n",
    "\n",
    "# image frame 불러오기\n",
    "img_1 = cv2.imread(f'example/{location}/{s}.jpg', cv2.IMREAD_COLOR) # 물 웅덩이 box 칠, 첫번째 이미지\n",
    "img_n = cv2.imread(f'example/{location}/{f}.jpg', cv2.IMREAD_COLOR) # n번째 이미지, 해당 코드에서 n=5\n",
    "\n",
    "std_cor = [] # 기준점 좌표\n",
    "seg_cor_1 = [] # 첫 프레임의 segment point 좌표\n",
    "ROI_cor = [] # 물 웅덩이 roi 좌표들\n",
    "car_result = [1.5] # 자동차 환산비율 평균 (임의)\n",
    "obj_result = [1.5] # 탐지 물체 환산비율 평균 (임의)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\heejae\\Flooding\n",
      "==============\n",
      "Car detected with bounding box corners: ((959, 169), (986, 169), (986, 196), (959, 196))\n",
      "Car detected with bounding box corners: ((511, 622), (714, 622), (714, 824), (511, 824))\n",
      "Car detected with bounding box corners: ((838, 291), (875, 291), (875, 341), (838, 341))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ROI 3: YOLO로 자동차 탐지 후 바운딩박스 생성\n",
    "\n",
    "변수\n",
    "unique_boxes: 바운딩 박스 좌표를 중복 저장하지 않기 위해 사용하는 set\n",
    "class_id: 자동차의 class_id = 6\n",
    "corners: YOLO 로 탐지한 bounding box의, 네 꼭짓점의 좌표\n",
    "\n",
    "동작 과정\n",
    "1. YOLO 로 첫번째 이미지 프레임에 있는 자동차 detect\n",
    "2. 탐지 결과를 file_path 와 result_image_path 에 저장\n",
    "3. 탐지 결과를 읽어들인 후, 조건에 맞는 객체 (자동차이며 신뢰도가 0.4 이상) 의 네 꼭짓점의 좌표 출력\n",
    "4. 탐지 결과 이미지 보여줌\n",
    "'''\n",
    "\n",
    "# YOLO detection 관련 경로 설정\n",
    "print(os.getcwd())\n",
    "command = [\n",
    "    './Algorithm/flooding/Scripts/python.exe', './YOLOv6-main/tools/infer.py',\n",
    "    '--weights', './YOLOv6-main/runs/1000epochs_exp31_best_ckpt.pt',\n",
    "    '--source', f'./example/' + location + '/4.jpg',\n",
    "    '--yaml', './yolo/dataset.yaml',\n",
    "    '--device', '0',\n",
    "    '--save-txt', \n",
    "    '--save-dir', f'./test_result/' + location + '/'\n",
    "]\n",
    "\n",
    "subprocess.run(command)\n",
    "print(\"==============\")\n",
    "\n",
    "file_path = f'./test_result/{location}/labels/{s}.txt'\n",
    "result_image_path = f'./test_result/{location}/{s}.jpg'\n",
    "result = []\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        elements = line.strip().split()\n",
    "        int_elements = list(map(float, elements))\n",
    "        result.append(int_elements)\n",
    "\n",
    "# 중복된 바운딩 박스 좌표를 저장하지 않도록 set 사용\n",
    "unique_boxes = set()\n",
    "\n",
    "cropped_car_images = []\n",
    "\n",
    "# 바운딩 박스의 좌표 계산 및 출력\n",
    "img_height, img_width = img_1.shape[:2]\n",
    "for line in result:\n",
    "    class_id = line[0]\n",
    "    confidence = line[5]\n",
    "    if class_id == 6 and confidence >= 0.4:\n",
    "        # YOLO 형식: class, x_center, y_center, width, height, confidence\n",
    "        x_center, y_center, width, height = line[1:5]\n",
    "\n",
    "        # 이미지 좌표계로 변환\n",
    "        x_center *= img_width\n",
    "        y_center *= img_height\n",
    "        width *= img_width\n",
    "        height *= img_height\n",
    "\n",
    "        # 바운딩 박스 좌표 (x_min, y_min, x_max, y_max)\n",
    "        x_min = int(x_center - width / 2)\n",
    "        y_min = int(y_center - height / 2)\n",
    "        x_max = int(x_center + width / 2)\n",
    "        y_max = int(y_center + height / 2)\n",
    "\n",
    "        # 네 꼭짓점 좌표\n",
    "        corners = ((x_min, y_min), (x_max, y_min), (x_max, y_max), (x_min, y_max))\n",
    "\n",
    "        # 중복되지 않은 바운딩 박스 좌표만 추가\n",
    "        if corners not in unique_boxes:\n",
    "            unique_boxes.add(corners)\n",
    "            # 좌표 출력\n",
    "            print(f\"Car detected with bounding box corners: {corners}\")\n",
    "\n",
    "            # 바운딩 박스 영역을 이미지에서 크롭\n",
    "            cropped_img = img_1[y_min:y_max, x_min:x_max]\n",
    "            \n",
    "            # 크롭된 이미지 저장\n",
    "            cropped_car_images.append(cropped_img)\n",
    "\n",
    "# 탐지된 결과를 보여주기\n",
    "detected_img = cv2.imread(f'test_result/{location}/{s}.jpg', cv2.IMREAD_COLOR)\n",
    "cv2.imshow('Detected Cars', detected_img)\n",
    "cv2.waitKey(0)  # 키 입력을 기다림\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO를 통해 탐지된 결과 하나씩 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롭된 자동차 이미지를 하나씩 보여주기\n",
    "for idx, cropped_img in enumerate(cropped_car_images):\n",
    "    # 이미지가 제대로 크롭되었는지 확인 후 표시\n",
    "    if cropped_img is not None and cropped_img.size > 0:\n",
    "        # 창 이름에 인덱스를 붙여 구분\n",
    "        window_name = f'Cropped Car {idx + 1}'\n",
    "        cv2.imshow(window_name, cropped_img)\n",
    "        \n",
    "        # 키 입력을 기다림 (0이면 무한 대기)\n",
    "        cv2.waitKey(0)\n",
    "        \n",
    "        # 창 닫기\n",
    "        cv2.destroyWindow(window_name)\n",
    "    else:\n",
    "        print(f\"Warning: Cropped Car {idx + 1} is None or empty\")\n",
    "\n",
    "# 모든 창 닫기\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\T719\\AppData\\Local\\Temp\\ipykernel_21072\\2060372243.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for Cropped Car 1: 2\n"
     ]
    }
   ],
   "source": [
    "# Device 설정 (GPU가 있으면 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 이미지 전처리\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((125, 125)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# VGG16 모델 초기화\n",
    "model = models.vgg16()\n",
    "\n",
    "# 마지막 레이어를 3개의 클래스를 예측하도록 수정\n",
    "model.classifier[6] = torch.nn.Linear(4096, 3)\n",
    "\n",
    "# 모델을 GPU 또는 CPU로 전송\n",
    "model = model.to(device)\n",
    "\n",
    "checkpoint_path = r'D:\\heejae\\Flooding\\module4\\checkpoint\\vgg16_final_weights.pth'  # 저장된 체크포인트 파일 경로\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint)  # 체크포인트에서 가중치 로드\n",
    "    model.eval()\n",
    "else:\n",
    "    print(\"Checkpoint 파일을 찾을 수 없습니다.\")\n",
    "    exit()\n",
    "\n",
    "# 이미지 배열 전처리\n",
    "def preprocess_image(cropped_img):\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n",
    "    img_tensor = preprocess(img_pil)\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)  # 배치 차원 추가 및 디바이스로 이동\n",
    "    return img_tensor\n",
    "\n",
    "# YOLO를 통해 탐지된 자동차 이미지에 대해 VGG16 모델 예측 \n",
    "def predict_vgg16(model, cropped_car_images):\n",
    "    class_names = ['0', '1', '2']  # 예시 클래스 이름 (필요 시 수정)\n",
    "    for idx, cropped_img in enumerate(cropped_car_images):\n",
    "        if cropped_img is not None and cropped_img.size > 0:\n",
    "            img_tensor = preprocess_image(cropped_img)\n",
    "            with torch.no_grad():\n",
    "                output = model(img_tensor)\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                predicted_class = class_names[predicted.item()]\n",
    "                print(f\"Predicted class for Cropped Car {idx + 1}: {predicted_class}\")\n",
    "        else:\n",
    "            print(f\"Warning: Cropped Car {idx + 1} is None or empty\")\n",
    "\n",
    "# YOLO로 탐지된 자동차 이미지 리스트(cropped_car_images)를 VGGNet에 입력하여 예측 실행\n",
    "predict_vgg16(model, cropped_car_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flooding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
