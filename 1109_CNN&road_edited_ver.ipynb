{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영상・이미지 데이터를 활용한 인공지능 기반 침수 위험 수준 탐지 기술 연구"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"image/Original_image.png\" width = 850 height = 750 alt=error></img>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수행 순서\n",
    "1. 사용자의 ROI 지정\n",
    "    - 기준점 (1개)\n",
    "    - water segmentation 기준점 (1개)\n",
    "    - 물 웅덩이 드래그 (2개)\n",
    "    - 도로 폭 (3쌍)\n",
    "2. YOLO 실행\n",
    "    - 기준객체 탐지\n",
    "    - 자동차 탐지\n",
    "3. SAM 실행 (water segmentation)\n",
    "4. Module #2 기준점 기반 침수 레벨 추정\n",
    "5. Module #3 웅덩이 기반 침수 레벨 추정\n",
    "6. Module #4 자동차 기반 침수 레벨 추정\n",
    "7. Module #5 Module#2, 3, 4 추정 레벨 output Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module #1. 사용자의 ROI 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"image/module1_ROI.png\" width = 500 height = 250 alt=error></img>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import subprocess\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from scipy.stats import hmean\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.models import VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 로드 및 location(폴더명) 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = \"동래구_삼환맨션_앞^20230711-150000-20230711-160000\"\n",
    "video_path = \"Dataset/test_case/\" + video_name + \".avi\" \n",
    "target_sec = [0, 2100] # [0, 45*60], 60 이 왜 곱해져있느지 의문! (sec 단위가 아니고 min 단위가 되어버림, sec 단위로 맞추기 위해 지웠습니다.)\n",
    "extracted_frame = []\n",
    "video_save_name = \"dongraegu_samhwanmension\"\n",
    "\n",
    "# 비디오 파일 열기\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# FPS와 총 프레임 수 가져오기\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "video_length_sec = total_frames / fps\n",
    "\n",
    "for i in target_sec:\n",
    "    # 지정한 초가 비디오 길이보다 길면 오류 메시지 출력\n",
    "    if i > video_length_sec:\n",
    "        print(f\"Error: 지정한 초({i})가 영상 길이({video_length_sec:.2f}초)를 초과합니다.\")\n",
    "    else:\n",
    "        # 초를 프레임으로 변환\n",
    "        target_frame = int(i * fps)\n",
    "\n",
    "        # 해당 프레임으로 이동하여 프레임 읽기\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            extracted_frame.append(frame)  # 추출된 프레임을 변수에 저장\n",
    "            print(f\"{i}초에 해당하는 프레임이 추출되었습니다.\")\n",
    "            os.makedirs('example/' + video_save_name + '/', exist_ok=True)\n",
    "            cv2.imwrite('example/' + video_save_name + '/' + str(i)+ '.jpg', frame)\n",
    "        else:\n",
    "            print(\"Error: 프레임을 읽는 데 실패했습니다.\")\n",
    "\n",
    "# 자원 해제\n",
    "cap.release()\n",
    "\n",
    "# 확인\n",
    "if 'extracted_frame' in locals():\n",
    "    print(f\"프레임이 변수 'extracted_frame'에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(25, 25))\n",
    "\n",
    "axes[0].imshow(cv2.cvtColor(extracted_frame[0], cv2.COLOR_BGR2RGB))\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Image 1 Original')\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(extracted_frame[1], cv2.COLOR_BGR2RGB))\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Image 1 Plotted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = video_name\n",
    "road_real_distance = 300 # 도로의 폭은 300cm 로 지정 / 만일 다른 기준객체를 참고하려면 해당 기준객체의 실 cm 값으로 변경\n",
    "lamp_real_distance = 400\n",
    "pier_real_distance = 200\n",
    "\n",
    "# s = 1 # 침수 전 이미지 프레임의 순서\n",
    "# f = 4 # 침수 후 이미지 프레임의 순서\n",
    "\n",
    "# image frame 불러오기\n",
    "img_1 = extracted_frame[0]\n",
    "img_n = extracted_frame[1]\n",
    "img_1_org = img_1.copy()\n",
    "img_n_org = img_n.copy()\n",
    "img_1_copy = img_1.copy()\n",
    "\n",
    "error_image = cv2.imread('example/error_image.png', cv2.IMREAD_COLOR)\n",
    "\n",
    "img_height, img_width = img_n_org.shape[:2]\n",
    "\n",
    "std_cor = [] # 기준점 좌표\n",
    "seg_cor_1 = [] # 첫 프레임의 segment point 좌표\n",
    "seg_cor_2 = [] # 두번째 프레임의 segment point 좌표\n",
    "ROI_cor = [] # 물 웅덩이 roi 좌표들\n",
    "# car_result = [1.5] # 자동차 환산비율 평균 (임의)\n",
    "# obj_result = [1.5] # 탐지 물체 환산비율 평균 (임의)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기준점 선택 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_event_seg(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        # print(f\"Clicked at x={x}, y={y}\")\n",
    "        cv2.putText(img_1, f'({x},{y})', (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0),  2, cv2.LINE_AA)\n",
    "        cv2.circle(img_1, (x, y), 8, (255, 0, 0), -1)  \n",
    "        cv2.imshow('image', img_1)\n",
    "        seg_cor_1.append(x)\n",
    "        seg_cor_1.append(y)\n",
    "\n",
    "def click_event_seg_2(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        # print(f\"Clicked at x={x}, y={y}\")\n",
    "        cv2.putText(img_1, f'({x},{y})', (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0),  2, cv2.LINE_AA)\n",
    "        cv2.circle(img_1, (x, y), 8, (255, 0, 0), -1)  \n",
    "        cv2.imshow('image', img_1)\n",
    "        seg_cor_2.append(x)\n",
    "        seg_cor_2.append(y)\n",
    "\n",
    "def click_event_std(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        # print(f\"Clicked at {x}, {y}, for std.\")\n",
    "        cv2.putText(img_1, f'({x},{y})', (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0),  2, cv2.LINE_AA)\n",
    "        cv2.circle(img_1, (x, y), 8, (255, 0, 0), -1)\n",
    "        cv2.imshow('image', img_1)\n",
    "        std_cor.append(x)\n",
    "        std_cor.append(y)\n",
    "\n",
    "drawing = False  # 드래그 상태를 확인하는 플래그\n",
    "# 마우스 콜백 함수\n",
    "def draw_rectangle(event, x, y, flags, param):\n",
    "    global drawing, water_start_x, water_start_y, water_width, water_height, water_final_x, water_final_y, img_1_copy\n",
    "    # print(\"start1\")\n",
    "    # 마우스 왼쪽 버튼을 누른 경우\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        water_start_x = x\n",
    "        water_start_y = y\n",
    "\n",
    "    # 마우스를 움직이는 중이고, 드래그 상태인 경우\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing:\n",
    "            img_1_copy = img_1.copy()\n",
    "            cv2.rectangle(img_1_copy, (water_start_x, water_start_y), (x, y), (0, 255, 0), 5)\n",
    "            cv2.imshow('image', img_1_copy)\n",
    "\n",
    "    # 마우스 왼쪽 버튼을 뗀 경우\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        water_final_x = min(water_start_x, x)\n",
    "        water_final_y = min(water_start_y, y)\n",
    "        water_width = abs(water_start_x - x)\n",
    "        water_height = abs(water_start_y - y)\n",
    "\n",
    "        # 네 꼭짓점 좌표 계산\n",
    "        top_left = (water_final_x, water_final_y)\n",
    "        top_right = (water_final_x + water_width, water_final_y)\n",
    "        bottom_left = (water_final_x, water_final_y + water_height)\n",
    "        bottom_right = (water_final_x + water_width, water_final_y + water_height)\n",
    "\n",
    "        # 네 꼭짓점 좌표 출력\n",
    "        ROI_cor.append(top_left)\n",
    "        ROI_cor.append(top_right)\n",
    "        ROI_cor.append(bottom_left)\n",
    "        ROI_cor.append(bottom_right)\n",
    "        # print(f\"Top-left: {top_left}\")\n",
    "        # print(f\"Top-right: {top_right}\")\n",
    "        # print(f\"Bottom-left: {bottom_left}\")\n",
    "        # print(f\"Bottom-right: {bottom_right}\")\n",
    "        # print(\"1\")\n",
    "\n",
    "def draw_rectangle2(event, x, y, flags, param):\n",
    "    global drawing, water_start_x, water_start_y, water_width, water_height, water_final_x, water_final_y, img_1_copy, img_1_copy2\n",
    "\n",
    "    # 마우스 왼쪽 버튼을 누른 경우\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        water_start_x = x\n",
    "        water_start_y = y\n",
    "\n",
    "    # 마우스를 움직이는 중이고, 드래그 상태인 경우\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing:\n",
    "            img_1_copy2 = img_1_copy.copy()\n",
    "            cv2.rectangle(img_1_copy2, (water_start_x, water_start_y), (x, y), (0, 255, 0), 5)\n",
    "            cv2.imshow('image', img_1_copy2)\n",
    "\n",
    "    # 마우스 왼쪽 버튼을 뗀 경우\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        water_final_x = min(water_start_x, x)\n",
    "        water_final_y = min(water_start_y, y)\n",
    "        water_width = abs(water_start_x - x)\n",
    "        water_height = abs(water_start_y - y)\n",
    "\n",
    "        # 네 꼭짓점 좌표 계산\n",
    "        top_left = (water_final_x, water_final_y)\n",
    "        top_right = (water_final_x + water_width, water_final_y)\n",
    "        bottom_left = (water_final_x, water_final_y + water_height)\n",
    "        bottom_right = (water_final_x + water_width, water_final_y + water_height)\n",
    "\n",
    "        # 네 꼭짓점 좌표 출력\n",
    "        ROI_cor.append(top_left)\n",
    "        ROI_cor.append(top_right)\n",
    "        ROI_cor.append(bottom_left)\n",
    "        ROI_cor.append(bottom_right)\n",
    "        # print(f\"Top-left: {top_left}\")\n",
    "        # print(f\"Top-right: {top_right}\")\n",
    "        # print(f\"Bottom-left: {bottom_left}\")\n",
    "        # print(f\"Bottom-right: {bottom_right}\")\n",
    "        # print(\"\")\n",
    "\n",
    "points = []  # 도로폭 픽셀 배열\n",
    "distances = []  # 도로폭의 L2 거리\n",
    "def draw_point_road(event, x, y, flags, param):\n",
    "    global road_ratio_har_result\n",
    "    \n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        points.append((x, y))\n",
    "        cv2.circle(img_1_copy2, (x, y), 8, (0, 0, 255), -1)\n",
    "        cv2.imshow('image', img_1_copy2)\n",
    "        \n",
    "        # 점이 적어도 두 개 이상일 때, 마지막으로 추가된 두 점의 거리를 계산하고 표시\n",
    "        if len(points) % 2 == 0:\n",
    "            pt1 = np.array(points[-2])\n",
    "            pt2 = np.array(points[-1])\n",
    "            \n",
    "            distance = np.linalg.norm(pt2 - pt1)  \n",
    "            distances.append(distance)\n",
    "            \n",
    "            # 거리를 이미지에 표시\n",
    "            mid_x = (pt1[0] + pt2[0]) // 2\n",
    "            mid_y = (pt1[1] + pt2[1]) // 2\n",
    "            # 거리와 선을 img_n에 표시\n",
    "            cv2.putText(img_1_copy2, f'{distance:.2f} px', (mid_x, mid_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            cv2.line(img_1_copy2, (pt1[0], pt1[1]), (pt2[0], pt2[1]), (0, 0, 255), 2)\n",
    "\n",
    "            cv2.imshow('image', img_1_copy2)\n",
    "            \n",
    "def convert_and_put_text(org_image, text, num): # 원본 이미지 / 입력할 텍스트 / 작동 순서 입력\n",
    "    image_convert = org_image\n",
    "    image_pil = Image.fromarray(image_convert)\n",
    "    draw = ImageDraw.Draw(image_pil)\n",
    "    draw.text(((img_width-50)/5, (img_height - 140)),  text , font=ImageFont.truetype(\"./font/NanumSquareB.ttf\", 50), fill=(255,255,255))\n",
    "    image_convert_2_numpy = np.array(image_pil)\n",
    "    # return cv2.cvtColor(image_convert_2_numpy, cv2.COLOR_RGB2BGR)\n",
    "    return image_convert_2_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용자의 기준점 선정\n",
    "> 클릭 후 키보드(어떤 값이던 상관없음)를 누르거나 \"X\" 를 눌러 창을 꺼야 다음 ROI 선택 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기준점 1개 클릭\n",
    "cv2.imshow('image', convert_and_put_text(img_1, \"1. 기준점 1개 클릭 후 키보드 아무 키나 누르세요\", 1))\n",
    "cv2.setMouseCallback('image', click_event_std)\n",
    "cv2.waitKey(0)\n",
    "cv2.imwrite('exam_img/image1.jpg', convert_and_put_text(img_1, \"1. 기준점 1개 클릭 후 키보드 아무 키나 누르세요\", 1))\n",
    "\n",
    "\n",
    "# water segmentation 기준점 1개 클릭\n",
    "cv2.imshow('image', convert_and_put_text(img_1, \"2. water segmentation 기준점 1개 클릭 후 키보드 아무 키나 누르세요\", 2))\n",
    "cv2.setMouseCallback('image', click_event_seg_2)\n",
    "cv2.waitKey(0)\n",
    "cv2.imwrite('exam_img/image2.jpg', convert_and_put_text(img_1, \"2. water segmentation 기준점 1개 클릭 후 키보드 아무 키나 누르세요\", 2))\n",
    "\n",
    "\n",
    "cv2.imshow('image', convert_and_put_text(img_n, \"3. water segmentation 기준점 1개 클릭 후 키보드 아무 키나 누르세요\", 3))\n",
    "cv2.setMouseCallback('image', click_event_seg)\n",
    "cv2.waitKey(0)\n",
    "cv2.imwrite('exam_img/image2_2.jpg', convert_and_put_text(img_n, \"3. water segmentation 기준점 1개 클릭 후 키보드 아무 키나 누르세요\", 3))\n",
    "\n",
    "# 웅덩이 드래그 1 \n",
    "\n",
    "cv2.imshow('image', convert_and_put_text(img_1, \"4. 첫번째 물 웅덩이 드래그 후 키보드 아무 키나 누르세요\", 4))\n",
    "cv2.setMouseCallback('image', draw_rectangle)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# 웅덩이 드래그 2\n",
    "\n",
    "cv2.imshow('image', convert_and_put_text(img_1_copy, \"5. 두번째 물 웅덩이 드래그 후 키보드 아무 키나 누르세요\", 5))\n",
    "cv2.setMouseCallback('image', draw_rectangle2)\n",
    "cv2.waitKey(0)\n",
    "cv2.imwrite('exam_img/image3.jpg',convert_and_put_text(img_1_copy, \"5. 첫번째 물 웅덩이 드래그 후 키보드 아무 키나 누르세요\", 5))\n",
    "cv2.imwrite('exam_img/image4.jpg',convert_and_put_text(img_1_copy, \"5. 두번째 물 웅덩이 드래그 후 키보드 아무 키나 누르세요\", 5))\n",
    "\n",
    "#도로 폭 3쌍의 점 클릭\n",
    "\n",
    "cv2.imshow('image', convert_and_put_text(img_1_copy2, \"6. 도로 폭 총 3쌍의 점 클릭 후 키보드 아무 키나 누르세요\", 6))\n",
    "cv2.setMouseCallback('image', draw_point_road)\n",
    "cv2.waitKey(0)\n",
    "cv2.imwrite('exam_img/image5.jpg', convert_and_put_text(img_1_copy2, \"6. 도로 폭 총 3쌍의 점 클릭 후 키보드 아무 키나 누르세요\", 6))\n",
    "cv2.imwrite('exam_img/image000.jpg', img_1_copy2)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  한번 실행 후 반드시 폴더 삭제할것 (덮어씌워짐)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"image/module1_SAM_YOLO.png\" width = 500 height = 250 alt=error></img>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기준객체 탐지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Custom YOLOv6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "YOLO로 기준객체 탐지 후 바운딩박스 생성\n",
    "\n",
    "동작 과정\n",
    "1. YOLO 로 첫번째 이미지 프레임에 있는 자동차 detect\n",
    "2. 탐지 결과를 file_path 와 result_image_path 에 저장\n",
    "3. 탐지 결과를 읽어들인 후, 조건에 맞는 객체 (자동차이며 신뢰도가 0.4 이상) 의 네 꼭짓점의 좌표 출력\n",
    "4. 탐지 결과 이미지 보여줌\n",
    "'''\n",
    "\n",
    "# YOLO detection 관련 경로 설정\n",
    "print(os.getcwd())\n",
    "command = [\n",
    "    './YOLOv6/venv_chim/Scripts/python.exe', './YOLOv6/tools/infer.py',\n",
    "    '--weights', './finetuned_models/custom_best_1027.pt',\n",
    "    '--source', f'./example/' + video_save_name + '/' + str(target_sec[0]) + '.jpg',\n",
    "    '--yaml', './finetuned_models/dataset_custom_1027.yaml',\n",
    "    '--device', '0',\n",
    "    '--save-txt', \n",
    "    '--save-dir', f'./object_detection_results/others/' + video_save_name + '/'\n",
    "]\n",
    "\n",
    "subprocess.run(command)\n",
    "print(\"==============\")\n",
    "\n",
    "# txt 파일이 존재하는지 확인하고 예외 처리\n",
    "if os.path.exists(f'object_detection_results/others/{video_save_name}/{target_sec[0]}.jpg'):\n",
    "    # 탐지된 결과를 보여주기\n",
    "    obj_detected_img = cv2.imread(f'object_detection_results/others/{video_save_name}/{target_sec[0]}.jpg', cv2.IMREAD_COLOR)\n",
    "    obj_detected_img = cv2.cvtColor(obj_detected_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # cv2.imshow('Detected Cars', detected_img)\n",
    "    # cv2.waitKey(0)  # 키 입력을 기다림\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    plt.imshow(obj_detected_img)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"탐지된 기준객체가 없습니다.\")\n",
    "    obj_detected_img=None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 자동차 탐지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> COCO pre-trained YOLOv6lv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "YOLO로 자동차 탐지 후 바운딩박스 생성\n",
    "\n",
    "변수\n",
    "unique_boxes: 바운딩 박스 좌표를 중복 저장하지 않기 위해 사용하는 set\n",
    "class_id: 자동차의 class_id = 6\n",
    "corners: YOLO 로 탐지한 bounding box의, 네 꼭짓점의 좌표\n",
    "\n",
    "동작 과정\n",
    "1. YOLO 로 첫번째 이미지 프레임에 있는 자동차 detect\n",
    "2. 탐지 결과를 file_path 와 result_image_path 에 저장\n",
    "3. 탐지 결과를 읽어들인 후, 조건에 맞는 객체 (자동차이며 신뢰도가 0.4 이상) 의 네 꼭짓점의 좌표 출력\n",
    "4. 탐지 결과 이미지 보여줌\n",
    "'''\n",
    "\n",
    "# YOLO detection 관련 경로 설정\n",
    "print(os.getcwd())\n",
    "command = [\n",
    "    './YOLOv6/venv_chim/Scripts/python.exe', './YOLOv6/tools/infer.py',\n",
    "    '--weights', './YOLOv6/yolov6l.pt',\n",
    "    '--source', f'./example/' + video_save_name + '/' + str(target_sec[1]) + '.jpg',\n",
    "    '--yaml', './YOLOv6/data/coco.yaml',\n",
    "    '--device', '0',\n",
    "    '--save-txt', \n",
    "    '--save-dir', f'./object_detection_results/cars/' + video_save_name + '/'\n",
    "]\n",
    "\n",
    "subprocess.run(command)\n",
    "print(\"==============\")\n",
    "\n",
    "file_path = f'./object_detection_results/cars/{video_save_name}/labels/{target_sec[1]}.txt'\n",
    "result_image_path = f'./object_detection_results/cars/{video_save_name}/{target_sec[1]}.jpg'\n",
    "result = []\n",
    "\n",
    "\n",
    "# txt 파일이 존재하는지 확인하고 예외 처리\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            elements = line.strip().split()\n",
    "            int_elements = list(map(float, elements))\n",
    "            result.append(int_elements)\n",
    "else:\n",
    "    print(\"탐지된 차량이 없습니다.\")\n",
    "    result = []\n",
    "    car_detected_img = None\n",
    "\n",
    "img_height, img_width = img_n_org.shape[:2]\n",
    "cropped_car_images = []\n",
    "if len(result) != 0:\n",
    "    # 중복된 바운딩 박스 좌표를 저장하지 않도록 set 사용\n",
    "    unique_boxes = set()\n",
    "\n",
    "    # 바운딩 박스의 좌표 계산 및 출력\n",
    "    for line in result:\n",
    "        class_id = line[0]\n",
    "        confidence = line[5]\n",
    "        if class_id == 2 and confidence >= 0.55:\n",
    "            # YOLO 형식: class, x_center, y_center, width, height, confidence\n",
    "            x_center, y_center, width, height = line[1:5]\n",
    "\n",
    "            # 이미지 좌표계로 변환\n",
    "            x_center *= img_width\n",
    "            y_center *= img_height\n",
    "            width *= img_width\n",
    "            height *= img_height\n",
    "\n",
    "            # 바운딩 박스 좌표 (x_min, y_min, x_max, y_max)\n",
    "            x_min = int(x_center - width / 2)\n",
    "            y_min = int(y_center - height / 2)\n",
    "            x_max = int(x_center + width / 2)\n",
    "            y_max = int(y_center + height / 2)\n",
    "\n",
    "            # 네 꼭짓점 좌표\n",
    "            corners = ((x_min, y_min), (x_max, y_min), (x_max, y_max), (x_min, y_max))\n",
    "\n",
    "            # 중복되지 않은 바운딩 박스 좌표만 추가\n",
    "            if corners not in unique_boxes:\n",
    "                unique_boxes.add(corners)\n",
    "                # 좌표 출력\n",
    "                print(f\"Car detected with bounding box corners: {corners}\")\n",
    "\n",
    "                # 바운딩 박스 영역을 이미지에서 크롭\n",
    "                cropped_img = img_n_org[y_min:y_max, x_min:x_max]\n",
    "                \n",
    "                # 크롭된 이미지 저장\n",
    "                cropped_car_images.append(cropped_img)\n",
    "\n",
    "    # 탐지된 결과를 보여주기\n",
    "    car_detected_img = cv2.imread(f'object_detection_results/cars/{video_save_name}/{target_sec[1]}.jpg', cv2.IMREAD_COLOR)\n",
    "    detected_img = cv2.cvtColor(car_detected_img, cv2.COLOR_BGR2RGB)\n",
    "    # cv2.imshow('Detected Cars', detected_img)\n",
    "    # cv2.waitKey(0)  # 키 입력을 기다림\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "    plt.imshow(car_detected_img)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "# sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"./finetuned_models/combined_model_1027.pth\"\n",
    "\n",
    "# 모델 유형 지점\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "sam = sam_model_registry[model_type]()\n",
    "# 크기가 맞지 않는 차원을 무시하고 state_dict를 로드하는 함수\n",
    "def load_partial_state_dict (model, state_dict):\n",
    "\tmodel_state = model.state_dict()\n",
    "\tfor name, param in state_dict.items():\n",
    "\t\tif name in model_state:\n",
    "\t\t\tif param.size() == model_state[name].size():\n",
    "\t\t\t\tmodel_state[name].copy_(param)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint (f\" {name}를 건너뜁니다: 모델은 {model_state[name].size()}를 기대하지만, 체크포인트는 {param.size()}를 제공합니다).\")\n",
    "\tmodel.load_state_dict (model_state, strict=False)\n",
    "\n",
    "# 체크포인트 로드, 모델 업데이트\n",
    "checkpoint = torch.load(sam_checkpoint, map_location='cpu')\n",
    "load_partial_state_dict (sam, checkpoint ['model']) \n",
    "sam.to(device='cuda')\n",
    "\n",
    "# predictor 인스턴스 생성\n",
    "predictor = SamPredictor (sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> water segmentation 수행 후 water mask 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = img_1_org\n",
    "input2 = img_n_org\n",
    "image1 = cv2.cvtColor(input1, cv2.COLOR_BGR2RGB)\n",
    "image2 = cv2.cvtColor(input2, cv2.COLOR_BGR2RGB)\n",
    "input_point = np.array([seg_cor_1])\n",
    "input_point_2 = np.array([seg_cor_2])\n",
    "input_label = np.array([1])\n",
    "\n",
    "\n",
    "predictor.set_image(image1)\n",
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=True,\n",
    ")\n",
    "\n",
    "img_1_mask = (masks[1] * 255).astype(np.uint8)  # 마스크를 0-255 범위로 변환\n",
    "\n",
    "\n",
    "predictor.set_image(image2)\n",
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point_2,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=True,\n",
    ")\n",
    "img_n_mask = (masks[1] * 255).astype(np.uint8)  # 마스크를 0-255 범위로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1_org_show = cv2.cvtColor(img_1_org, cv2.COLOR_BGR2RGB)\n",
    "img_n_org_show = cv2.cvtColor(img_n_org, cv2.COLOR_BGR2RGB)\n",
    "img_1_copy2_show = cv2.cvtColor(img_1_copy2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "\n",
    "axes[0].imshow(img_1_org_show)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Image 1 Original')\n",
    "\n",
    "axes[1].imshow(img_1_copy2_show)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Image 1 Plotted')\n",
    "\n",
    "axes[2].imshow(img_1_mask, cmap = 'gray')\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Image 1 Masked')\n",
    "\n",
    "axes[3].imshow(img_n_org_show)\n",
    "axes[3].axis('off')\n",
    "axes[3].set_title('Image n Original')\n",
    "\n",
    "axes[4].imshow(img_n_mask, cmap = 'gray')\n",
    "axes[4].axis('off')\n",
    "axes[4].set_title('Image n Masked')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(std_cor, ROI_cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module #2. 기준점-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"image/module2.png\" width = 300 height = 300 alt=error></img>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"image/module1_point.png\" width = 1000 height = 250 alt=error></img>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_water_px_dis = []        # 물의 증가값 list (PX)\n",
    "h_mean_distance = hmean(distances)\n",
    "road_ratio_har_result = road_real_distance / h_mean_distance  # 도로 폭의 조화평균 구하기 (h_mean_distance)\n",
    "print(road_ratio_har_result)\n",
    "total_ratio_road = road_ratio_har_result    # 도로 폭 기반 비율값 list (cm/px)\n",
    "total_ratio_obj = None     # yolo 로 탐지된 기준객체 전체 기준객체 비율 평균값 (cm/px)\n",
    "total_obj_based_cm = []\n",
    "total_road_based_cm = []\n",
    "module2_results = []    # module #2 추정 침수 레벨 list (level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 기준객체 기반 비율 계산 (cm/px)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!!!!!!! 신뢰도 0.5 이상으로 수정할 것!!!! (현재는 테스트를 위해 낮게 설정함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO 사용한 기준객체 비율 계산\n",
    "obj_ratio_result = []  # 각 사진 별 기준객체 픽셀 수 계산 리스트\n",
    "\n",
    "\n",
    "# 위에서 YOLO detection 결과를 저장한 path\n",
    "file_path = f'object_detection_results/others/{video_save_name}/labels/{target_sec[0]}.txt'\n",
    "\n",
    "obj_detect_result = []\n",
    "\n",
    "\n",
    "# YOLO detect 한 바운딩박스 값이 저장된 txt 파일 open\n",
    "if os.path.isfile(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # 각 줄의 양 끝 공백을 제거하고, 띄어쓰기를 기준으로 분할하여 리스트로 저장\n",
    "            elements = line.strip().split()\n",
    "            int_elements = list(map(float, elements))\n",
    "            obj_detect_result.append(int_elements)\n",
    "\n",
    "    # line 의 index 0: 기준객체의 클래스 number\n",
    "    # line 의 index 5: 기준객체 탐지 신뢰도\n",
    "    # line 의 index 4: 탐지된 기준객체의 height 값 (px)\n",
    "    for line in obj_detect_result:\n",
    "        # print(line)\n",
    "        if line[0] == 0 and line[5] >= 0.3:  # 교각 and 신뢰도 0.3 이상의 detection만\n",
    "            # 교각 평균 높이.................................... -> 2m\n",
    "            obj_ratio_result.append( pier_real_distance / (img_height * line[4]))\n",
    "        if line[0] == 1 and line[5] >= 0.3:  # 가로등 and 신뢰도 0.3 이상의 detection만\n",
    "            # 가로등 평균 높이.................................... -> 4.m\n",
    "            obj_ratio_result.append( lamp_real_distance / (img_height * line[4]))\n",
    "        # if line[0] == 6 and line[5] >= 0.3:  # 자동차 and 신뢰도 0.3 이상의 detection만\n",
    "        #     # 자동차 평균 높이.................................... -> 1.5m\n",
    "        #     obj_ratio_result.append( 150 / (img_height * line[4]))\n",
    "        # if line[0] == 7 and line[5] >= 0.3:  # 전봇대 and 신뢰도 0.3 이상의 detection만\n",
    "        #     # 전봇대 평균 높이.................................... -> 14m\n",
    "        #     obj_ratio_result.append( 1400 / (img_height * line[4]))\n",
    "    # print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    # print(obj_ratio_result)\n",
    "    print(\"obj_ratio_result:\", obj_ratio_result)\n",
    "\n",
    "    # yolo 로 탐지된 기준객체 기반 비율 계산 (cm/px)\n",
    "    obj_ratio_result = np.array(obj_ratio_result)\n",
    "    # total_ratio_obj.append(len(obj_ratio_result) / np.sum(1.0 / obj_ratio_result) if len(obj_ratio_result) > 0 else 0)  # 조화 평균 사용\n",
    "\n",
    "    total_ratio_obj = np.mean(obj_ratio_result) # 평균내기\n",
    "    print(\"total_ratio_obj: \", total_ratio_obj)\n",
    "\n",
    "else:\n",
    "    print(\"참고할 기준객체가 없습니다.\")\n",
    "    total_ratio_obj = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 기준점과 water mask 간의 거리(px) 계산 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_white_point(img, x, y_start):\n",
    "    \"\"\"\n",
    "    이미지에서 특정 x 좌표에 대해 y_start부터 내려가며 흰색(255) 부분을 찾는 함수\n",
    "    img: 이미지 (numpy 배열)\n",
    "    x: 수직선을 그릴 x 좌표\n",
    "    y_start: 검색을 시작할 y 좌표\n",
    "    return: 처음으로 흰색(255) 부분과 만나는 (x, y) 좌표\n",
    "    \"\"\"\n",
    "    height = img.shape[0]  # 이미지의 세로 길이 (y 축의 최대 값)\n",
    "\n",
    "    # y_start부터 이미지의 끝까지 내려가면서 흰색 부분(255)이 있는지 확인\n",
    "    for y in range(y_start, height):\n",
    "        if img[y, x] == 255:  # 흰색 부분을 만나면\n",
    "            return (x, y)\n",
    "\n",
    "    # 흰색 부분을 만나지 못하면 None 리턴\n",
    "    return (300000, 300000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기준점 (1개) 기반 침수 높이 추정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 지정 기준점(1개) 기반 침수 레벨 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std_cor 리스트에 있는 각 좌표에 대해 img1과 img2에서 흰색 좌표를 찾기\n",
    "point_img1 = []\n",
    "point_img2 = []\n",
    "\n",
    "\n",
    "x, y_start = std_cor  # std_cor에서 x와 시작 y 좌표를 추출\n",
    "print(x, y_start)\n",
    "\n",
    "# img1에서 흰색 부분 찾기\n",
    "point1 = find_white_point(img_1_mask, x, y_start)\n",
    "if point1:\n",
    "    point_img1.append(point1[0])\n",
    "    point_img1.append(point1[1])\n",
    "\n",
    "# img2에서 흰색 부분 찾기\n",
    "point2 = find_white_point(img_n_mask, x, y_start)\n",
    "if point2:\n",
    "    point_img2.append(point2[0])\n",
    "    point_img2.append(point2[1])\n",
    "\n",
    "# 결과: point_img1, point_img2 에 각각 흰색 좌표들이 저장됨\n",
    "print(\"mask 1 에서 물과 맞닿는 좌표:\", point_img1)\n",
    "print(\"mask n 에서 물과 맞닿는 좌표:\", point_img2)\n",
    "if point2==None or point1==None or point_img1[0]==300000 or point_img2[0]==300000:\n",
    "    print(\"물의 증가 픽셀 구할 수 없음\")\n",
    "    total_water_px_dis.append(300000)\n",
    "    \n",
    "else:\n",
    "    # mask 에서 물의 증가 거리 구하기 (px)\n",
    "    total_water_px_dis.append(point_img1[1]-point_img2[1])\n",
    "    print(\"물의 증가 px:\", total_water_px_dis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 침수 속도 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sec\n",
    "flood_speed = []\n",
    "if total_water_px_dis[0] == 30000:\n",
    "    print(\"침수 속도 추정 불가\")\n",
    "elif total_ratio_obj != 0 or total_ratio_road != 0:\n",
    "    # 물 영역 증가 px 과 기준객체 기반 환산비율 곱하기\n",
    "    flood_speed.append(total_water_px_dis[0]*total_ratio_obj)\n",
    "    # 물 영역 증가 px 과 도로 폭 기반 환산비율 곱하기\n",
    "    flood_speed.append(total_water_px_dis[0]*total_ratio_road)\n",
    "\n",
    "total_flood_speed =  ( ( flood_speed[0] * 1 + flood_speed[1] * 2 ) / 3 ) / (target_sec[1] - target_sec[0])\n",
    "\n",
    "print(f\"침수 속도 {round(total_flood_speed, 3)}(cm/sec)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 코드 완성되면 삭제할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if point2==None or point1==None:\n",
    "    print(\"X\")\n",
    "else:\n",
    "    # 이미지 시각화\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # 첫 번째 이미지 시각화\n",
    "    ax[0].imshow(img_1_mask, cmap='gray')\n",
    "    ax[0].scatter(point_img1[0], point_img1[1], color='red', label='The point where the base point and the water mask meet', s=100)\n",
    "    ax[0].scatter(x, y_start, color='blue', label='Custom base point', s=100)\n",
    "    ax[0].set_title('Image 1 with Points')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # 두 번째 이미지 시각화\n",
    "    ax[1].imshow(img_n_mask, cmap='gray')\n",
    "    ax[1].scatter(point_img2[0], point_img2[1], color='red', label='The point where the base point and the water mask meet', s=100)\n",
    "    ax[1].scatter(x, y_start, color='blue', label='Custom base point', s=100)\n",
    "    ax[1].set_title('Image N with Points')\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용자 지정 기준점(1개) + 그 주변 점(2개) 기반 침수 레벨 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직접 찍은 기준점과 그 근처 점(2개) -  물 사이의 거리 구하기\n",
    "\n",
    "# std_cor 리스트에 있는 각 좌표에 대해 img1과 img2에서 흰색 좌표를 찾기\n",
    "point_img1 = []\n",
    "point_img2 = []\n",
    "\n",
    "\n",
    "x, y_start = std_cor  # std_cor에서 x와 시작 y 좌표를 추출\n",
    "print(x, y_start)\n",
    "\n",
    "for i in range(3):\n",
    "    # img1에서 흰색 부분 찾기\n",
    "    point1 = find_white_point(img_1_mask, x, y_start)\n",
    "    if point1:\n",
    "        # point_img1.append(point1[0])\n",
    "        point_img1.append(point1[1])\n",
    "\n",
    "    # img2에서 흰색 부분 찾기\n",
    "    point2 = find_white_point(img_n_mask, x, y_start)\n",
    "    if point2:\n",
    "        # point_img2.append(point2[0])\n",
    "        point_img2.append(point2[1])\n",
    "    x += 60\n",
    "\n",
    "\n",
    "# 결과: point_img1, point_img2 에 각각 흰색 좌표들이 저장됨\n",
    "print(\"mask 1 에서 물과 맞닿는 좌표:\", point_img1)\n",
    "print(\"mask n 에서 물과 맞닿는 좌표:\", point_img2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if point2==None or point1==None:\n",
    "    print(\"X\")\n",
    "    \n",
    "else:\n",
    "    # 이미지 시각화\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # 첫 번째 이미지 시각화\n",
    "    ax[0].imshow(img_1_mask, cmap='gray')\n",
    "\n",
    "    x = std_cor[0]\n",
    "    # point_img1 시각화\n",
    "    for i in range(0, len(point_img1), 2):  # 2씩 증가하며 점을 묶어 시각화\n",
    "        ax[0].scatter(point_img1[i], point_img1[i], color='red', s=100)\n",
    "        # std_cor 시각화 (이동한 위치)\n",
    "        ax[0].scatter(x, std_cor[1] , color='blue', label='std_cor', s=100)\n",
    "        x += 60\n",
    "    ax[0].set_title('Image 1 with Points')\n",
    "    ax[0].legend()\n",
    "\n",
    "    # 두 번째 이미지 시각화\n",
    "    ax[1].imshow(img_n_mask, cmap='gray')\n",
    "\n",
    "    x = std_cor[0]\n",
    "    # point_img2 시각화\n",
    "    for i in range(0, len(point_img2), 2):  # 2씩 증가하며 점을 묶어 시각화\n",
    "        ax[1].scatter(point_img2[i], point_img2[i], color='red', s=100)\n",
    "        # std_cor 시각화 (이동한 위치)\n",
    "        ax[1].scatter(x, std_cor[1], color='blue', label='std_cor', s=100)\n",
    "        x += 60\n",
    "    ax[1].set_title('Image N with Points')\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "point_water_dis=[]\n",
    "# three_point_road = []\n",
    "# three_point_obj = []\n",
    "\n",
    "if point2==None or point1==None:\n",
    "    print(\"X\")\n",
    "    total_water_px_dis.append(300000)\n",
    "else:\n",
    "    for i in range(len(point_img1)):\n",
    "        if point_img1[i] != 300000 and  point_img2[i] != 300000:\n",
    "            # mask 에서 물의 증가 거리 구하기 (px)\n",
    "            point_water_dis.append(point_img1[i]-point_img2[i])\n",
    "        else:\n",
    "            print(f\"기준점 {i}번째 점 물의 증가값 구할 수 없음\")\n",
    "\n",
    "if len(point_water_dis) != 0:\n",
    "    print(f\"기준점 {len(point_water_dis)}개 기반 물의 증가 px:\", point_water_dis)\n",
    "    # print(\"도로 폭 기반 침수 깊이 (cm): \", mean(three_point_road))\n",
    "    # print(\"기준객체 기반 침수 깊이 (cm): \", mean(three_point_obj))\n",
    "    print(f\"기준점 {len(point_water_dis)}개 기반 물의 증가 px 평균값:\", mean(point_water_dis))\n",
    "    total_water_px_dis.append(mean(point_water_dis))\n",
    "    # 가중치 계산\n",
    "    # 도로 폭 가중치: 3\n",
    "    # 기준객체 가중치: 1\n",
    "    # module2_results.append((mean(three_point_road) * 3 + mean(three_point_obj)* 1 ) / 4)\n",
    "    # print(module2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_water_px_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마스크 전체 조화평균 기반 침수 레벨 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 경로 확인 및 로드\n",
    "img_1_array = np.array(img_1_mask)\n",
    "img_n_array = np.array(img_n_mask)\n",
    "\n",
    "mask_height, mask_width, = img_1_array.shape\n",
    "height = []\n",
    "x_array = []\n",
    "\n",
    "# 침수 비교 영역 설정\n",
    "for x in range(mask_width):\n",
    "    for y in range(mask_height):\n",
    "        if img_n_array[y, x] > 200:  # 침수 후 영역의 x 값이\n",
    "            if img_1_array[y, x] > 200:  # 침수 전 영역의 x 값에도 있다면\n",
    "                x_array.append(x)  # 올바른 침수 비교 영역으로 설정\n",
    "                break\n",
    "\n",
    "print(f\"Valid X coordinates count: {len(x_array)}\")\n",
    "\n",
    "# y 값의 증가 값 계산\n",
    "for x in x_array:\n",
    "    for y in range(mask_height):\n",
    "        if img_1_array[y, x] > 200:\n",
    "            a = y  # 침수 비교 영역의 x값에 대한 가장 높은 침수 전 영역의 y 값\n",
    "            break\n",
    "    for y in range(mask_height):\n",
    "        if img_n_array[y, x] > 200:\n",
    "            b = y  # 침수 비교 영역의 x값에 대한 가장 높은 침수 후 영역의 y 값\n",
    "            break\n",
    "    height.append(a - b)  # y 값의 증가 값\n",
    "\n",
    "height = np.array(height)\n",
    "\n",
    "# 조화 평균 계산\n",
    "if len(height) > 0 and np.sum(height > 0) > 0:  # 0으로 나누는 경우 방지\n",
    "    harmonic_height_mask = len(height) / np.sum(1.0 / height[height > 0])\n",
    "    print(f\"Mask's Harmonic Flood Height(px): {harmonic_height_mask}\")\n",
    "    total_water_px_dis.append(harmonic_height_mask)\n",
    "else:\n",
    "    harmonic_height_mask = 0\n",
    "    print(\"No valid height data for harmonic mean calculation.\")\n",
    "    total_water_px_dis.append(300000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_water_px_dis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> mask 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_add(image, mask, color):\n",
    "    # mask 이미지를 지정한 색으로으로 변환하고, 검정색 부분을 투명하게 만든다\n",
    "    purple_mask = np.zeros_like(image)\n",
    "    purple_mask[mask > 0] = color\n",
    "\n",
    "    # 알파 채널 추가 (투명도 설정)\n",
    "    alpha_channel = np.zeros_like(mask)\n",
    "    alpha_channel[mask > 0] = 255\n",
    "    purple_mask = cv2.merge((*cv2.split(purple_mask), alpha_channel))\n",
    "\n",
    "    # 원본 이미지에도 알파 채널 추가\n",
    "    original_image = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "    # 두 이미지를 합친다\n",
    "    composited_image = cv2.addWeighted(original_image, 1, purple_mask, 1, 0)\n",
    "    return composited_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_water_px_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ratio_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(total_water_px_dis)):\n",
    "    if total_water_px_dis[i] != 300000 and total_water_px_dis[i] > 0:\n",
    "        if i < 2:\n",
    "            print(f\"기준점 {i*2+1}개 기반 물 증가 픽셀값: {total_water_px_dis[i]} PX\")\n",
    "            total_obj_based_cm.append(total_water_px_dis[i]*total_ratio_obj)\n",
    "            total_road_based_cm.append(total_water_px_dis[i]*total_ratio_road)\n",
    "        else:\n",
    "            print(f\"마스크 조화평균 기반 물 증가 픽셀값: {total_water_px_dis[i]} PX\")\n",
    "            total_obj_based_cm.append(total_water_px_dis[i]*total_ratio_obj)\n",
    "            total_road_based_cm.append(total_water_px_dis[i]*total_ratio_road)\n",
    "\n",
    "    else:\n",
    "        if i == 2:\n",
    "            print(\"마스크 조화평균 기반 침수 추정 불가능\")\n",
    "        else:\n",
    "            print(f\"기준점 {i*2 + 1}개 기반 침수 추정 불가능\")\n",
    "            \n",
    "\n",
    "print(\"total_water_px_dis\", total_water_px_dis)\n",
    "print(\"total_ratio_obj\", total_ratio_obj)\n",
    "print(\"total_ratio_road\", total_ratio_road)\n",
    "print(\"total_obj_based_cm\", total_obj_based_cm)\n",
    "print(\"total_road_based_cm\", total_road_based_cm)\n",
    "\n",
    "\n",
    "    # # 가중치 계산\n",
    "    # # 도로 폭 가중치: 3\n",
    "    # # 기준객체 가중치: 1\n",
    "    # module2_results.append((har_total[0] * 3 + har_total[1]* 1 ) / 4)\n",
    "    # print(module2_results)\n",
    "\n",
    "\n",
    "if total_ratio_obj == 0:    # 기준객체 없는 경우\n",
    "    module2_results = total_road_based_cm\n",
    "    print(module2_results)\n",
    "else:\n",
    "    module2_results = [(road * 3 + obj * 1) / 4 for road, obj in zip(total_road_based_cm, total_obj_based_cm)]\n",
    "    print(module2_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 침수 속도 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_speed = module2_results[0] / (target_sec[1]-target_sec[0])\n",
    "print(f\"침수 속도 추정 결과: {np.round(flood_speed, 3)} cm/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module2_results_level = []\n",
    "if len(module2_results) == 0:\n",
    "    print(\"침수 레벨 추정 불가\")\n",
    "\n",
    "else:\n",
    "    # 계산된 값(cm) 에 대한 레벨 산정하는 코드\n",
    "\n",
    "    # cm -> level 변환 (임의)\n",
    "    # 0 -> level 0\n",
    "    # 1 ~ 50 -> level 1\n",
    "    # 51 ~ 100 -> level 2\n",
    "    # 101 ~ 200 -> level 3\n",
    "    # 201 ~ -> level 4\n",
    "    for i in range(len(module2_results)):\n",
    "        if module2_results[i] == 0:\n",
    "            module2_results_level.append(0)\n",
    "        elif 1 <= module2_results[i] <= 50:\n",
    "            module2_results_level.append(1)\n",
    "        elif 51 <= module2_results[i] <= 100:\n",
    "            module2_results_level.append(2)\n",
    "        elif 101 <= module2_results[i] <= 200:\n",
    "            module2_results_level.append(3)\n",
    "        elif 201 <= module2_results[i]:\n",
    "            module2_results_level.append(4)\n",
    "\n",
    "    # 각 방법 별 가중치 두기 위해 순서 바꿈\n",
    "    module2_results_level.reverse()\n",
    "\n",
    "    print(\"Module#2 기준점-based 결과\")\n",
    "    print(\"침수 추정 레벨: \", module2_results_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module#3 웅덩이-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"image/module3.png\" width = 300 height = 300 alt=error></img>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module3_results_level = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_n_mask = (masks[1] * 255).astype(np.uint8)  # 마스크를 0-255 범위로 변환\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "for i in range(2):\n",
    "    roi = img_n_mask[ROI_cor[(i*4+0)][1]:ROI_cor[(i*4+2)][1], ROI_cor[(i*4+0)][0]:ROI_cor[(i*4+1)][0]]\n",
    "    roi_ori = img_n_org[ROI_cor[(i*4+0)][1]:ROI_cor[(i*4+2)][1], ROI_cor[(i*4+0)][0]:ROI_cor[(i*4+1)][0]]\n",
    "    percentage = (np.sum(roi)/255)/((ROI_cor[i*4][0]-ROI_cor[i*4+1][0])*(ROI_cor[i*4][1]-ROI_cor[i*4+2][1]))\n",
    "    axes[i][0].imshow(roi_ori)\n",
    "    axes[i][0].axis('off')\n",
    "    axes[i][0].set_title(f'Original Image ROI: {percentage:f}')\n",
    "\n",
    "    axes[i][1].imshow(roi, cmap='gray')\n",
    "    axes[i][1].axis('off')\n",
    "    axes[i][1].set_title('Masked Image ROI')\n",
    "\n",
    "\n",
    "    \n",
    "    if percentage > 0.5:\n",
    "        print('level: 2')\n",
    "        module3_results_level.append(2)\n",
    "    elif percentage > 0:\n",
    "        print('level: 1')\n",
    "        module3_results_level.append(1)\n",
    "    else:\n",
    "        print('level: 0')\n",
    "        module3_results_level.append(0)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(module3_results_level)\n",
    "\n",
    "# 첫 번째 값을 복사해서 두 번째 위치에 삽입\n",
    "module3_results_level.insert(1, module3_results_level[0])\n",
    "print(module3_results_level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module#4. 자동차-based method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"image/module4.png\" width = 300 height = 300 alt=error></img>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module4_results_level = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device 설정 (GPU가 있으면 사용)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 이미지 전처리\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# 이미지 배열 전처리\n",
    "def preprocess_image(cropped_img):\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB))\n",
    "    img_tensor = preprocess(img_pil)\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)  # 배치 차원 추가 및 디바이스로 이동\n",
    "    return img_tensor\n",
    "\n",
    "\n",
    "def predict_cnn(model, cropped_car_images):\n",
    "    predicted_classes = []\n",
    "    predicted_probs = []\n",
    "\n",
    "    with torch.no_grad():  # 예측 시에는 역전파 계산이 필요 없으므로 no_grad() 사용\n",
    "        for img in cropped_car_images:\n",
    "            # 이미지 전처리: 필요한 경우 크기 조정, 정규화 등 수행\n",
    "            img = preprocess_image(img)  # 필요 시 사용자 정의 이미지 전처리 함수 사용\n",
    "            img = img.to(device)  # 이미지를 GPU 또는 CPU로 전송\n",
    "            # img = img.unsqueeze(0)  # 배치 차원을 추가 (1, C, H, W 형태로 변경)\n",
    "\n",
    "            # 모델로 예측 수행\n",
    "            outputs = model(img)\n",
    "\n",
    "            # Softmax를 적용하여 각 클래스에 대한 확률을 구함\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            # 가장 높은 확률을 가진 클래스 선택\n",
    "            _, predicted_class = torch.max(probabilities, 1)\n",
    "            \n",
    "            # 예측된 클래스 및 확률 저장\n",
    "            predicted_classes.append(predicted_class.item())\n",
    "            predicted_probs.append(probabilities.cpu().numpy())  # 확률 값을 numpy로 변환하여 저장\n",
    "    return predicted_classes, predicted_probs\n",
    "\n",
    "def prop_sorted_pre_class(A, B):    # 예측된 클래스 / 예측된 확률을 순서대로 입력\n",
    "    prob_list = []\n",
    "    sorted_class_list = []\n",
    "    for i in range(len(A)):\n",
    "        prob_list.append((B[i][0][A[i]], A[i]))\n",
    "    # print(prob_list)\n",
    "    prob_list.sort(reverse=True)\n",
    "    #print(prob_list)\n",
    "    for i in range(len(prob_list)):\n",
    "        sorted_class_list.append(prob_list[i][1] + 1)\n",
    "    if len(sorted_class_list) == 1:\n",
    "        sorted_class_list.insert(1, sorted_class_list[0])\n",
    "        sorted_class_list.insert(2, sorted_class_list[0])\n",
    "    elif len(sorted_class_list) == 2:\n",
    "        sorted_class_list.insert(1, sorted_class_list[0])\n",
    "    #print(sorted_class_list)\n",
    "    return sorted_class_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(cropped_car_images) == 0:\n",
    "    print(\"there's no detected car\")\n",
    "    \n",
    "else:\n",
    "    path = './finetuned_models/model_checkpoint_431_0.7600.pt'\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        model = torch.load(path).to(device)\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        model.eval()\n",
    "    else:\n",
    "        print(\"Checkpoint 파일을 찾을 수 없습니다.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    pred_class, pred_prob = predict_cnn(model, cropped_car_images)\n",
    "    module4_results_level = prop_sorted_pre_class(pred_class, pred_prob)\n",
    "    print(\"Module #4  차량 추정 레벨: \", module4_results_level)\n",
    "    \n",
    "    cropped_car_count = len(cropped_car_images)\n",
    "    fig, axes = plt.subplots(1, cropped_car_count, figsize=((cropped_car_count*5), 5))\n",
    "\n",
    "    # 만약 서브플롯이 1개일 경우 axes는 단일 객체이므로 배열로 변환\n",
    "    if cropped_car_count == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(cropped_car_count):\n",
    "        # 이미지가 제대로 크롭되었는지 확인 후 표시\n",
    "        cropped_car_images[i] = cv2.cvtColor(cropped_car_images[i], cv2.COLOR_BGR2RGB)\n",
    "        if cropped_car_images[i] is not None and cropped_car_images[i].size > 0:\n",
    "            axes[i].imshow(cropped_car_images[i])\n",
    "            axes[i].axis('off')\n",
    "            axes[i].set_title(f'Image {i+1} Predicted Level: {pred_class[i]+1}')\n",
    "        else:\n",
    "            print(f\"Warning: Cropped Car Image {i+1} is None or empty\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module #5 Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"image/module5.png\" width = 300 height = 250 alt=error></img>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"module #2 output: \", module2_results_level)\n",
    "print(\"module #3 output: \", module3_results_level)\n",
    "print(\"module #4 output: \", module4_results_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# (앞에서 Output 받았다고 가정)\n",
    "# module2_results_level = [3, 4, 3]  \n",
    "# module3_results_level = [2, 2, 2]  \n",
    "# module4_results_level = [4, 3, 2]  \n",
    "\n",
    "# 각 모듈 별 가중치 설정\n",
    "module_weights = {\n",
    "    'module2': 1.5,\n",
    "    'module3': 1.0,\n",
    "    'module4': 1.0\n",
    "}\n",
    "\n",
    "# module3_results_level의 값 확인 (최대 레벨 출력이 2 이기 때문에, 모두 2로 출력될 경우, 가중치 조절)\n",
    "if all(value == 2 for value in module3_results_level):\n",
    "    high_value_count = sum(1 for value in module2_results_level + module4_results_level if value >= 3)\n",
    "    if high_value_count >= 3:\n",
    "        module_weights['module3'] = 0.25\n",
    "    elif high_value_count == 2:\n",
    "        module_weights['module3'] = 0.5\n",
    "\n",
    "# 모듈 별 가중치 및 모듈 Output의 각 인덱스 별 가중치 적용 함수\n",
    "def apply_weights(values, base_weight):\n",
    "    weights = [int(base_weight * 2) if i == 0 else 1 for i in range(len(values))]\n",
    "    weighted_values = []\n",
    "    for value, weight in zip(values, weights):\n",
    "        weighted_values.extend([value] * weight)\n",
    "    return weighted_values\n",
    "\n",
    "# 각 모듈별 가중치 적용\n",
    "weighted_module2 = apply_weights(module2_results_level, module_weights['module2'])\n",
    "weighted_module3 = apply_weights(module3_results_level, module_weights['module3'])\n",
    "weighted_module4 = apply_weights(module4_results_level, module_weights['module4'])\n",
    "\n",
    "# 모듈별 Output에 대한 가중치 적용 결과 합산\n",
    "all_votes = weighted_module2 + weighted_module3 + weighted_module4\n",
    "\n",
    "# 다수결로 최종 침수 레벨 결정\n",
    "vote_counts = Counter(all_votes)\n",
    "final_level = vote_counts.most_common(1)[0][0]\n",
    "\n",
    "print(f\"최종 침수 레벨: {final_level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선택한 ROI, water mask, 탐지된 자동차 및 기준객체 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1_add_mask = cv2.cvtColor(mask_add(img_1_org, img_1_mask, (0, 100, 55)), cv2.COLOR_BGR2RGB)\n",
    "img_n_add_mask = cv2.cvtColor(mask_add(img_n_org, img_n_mask, (0, 100, 55)), cv2.COLOR_BGR2RGB)\n",
    "img_1_copy2_show = cv2.cvtColor(img_1_copy2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "\n",
    "axes[0].imshow(img_1_add_mask)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Image 1 add Water Mask')\n",
    "\n",
    "axes[1].imshow(img_n_add_mask)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Image N add Water Mask')\n",
    "\n",
    "axes[2].imshow(img_1_copy2_show)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('selected ROI')\n",
    "\n",
    "if car_detected_img is not None and car_detected_img.size:\n",
    "    axes[3].imshow(car_detected_img)\n",
    "    axes[3].axis('off')\n",
    "    axes[3].set_title('Detected Cars')\n",
    "else:\n",
    "    axes[3].imshow(error_image)\n",
    "    axes[3].axis('off')\n",
    "    axes[3].set_title('There are no detected car')\n",
    "\n",
    "\n",
    "if obj_detected_img is not None and obj_detected_img.size > 0:\n",
    "    axes[4].imshow(obj_detected_img)\n",
    "    axes[4].axis('off')\n",
    "    axes[4].set_title('Detected Reference Object')\n",
    "else:\n",
    "    axes[4].imshow(error_image)\n",
    "    axes[4].axis('off')\n",
    "    axes[4].set_title('탐지된 참조객체 없음')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"최종 침수 레벨: {final_level}\")\n",
    "print(f\"침수 속도 추정 결과: {np.round(flood_speed, 3)} cm/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"image/level.png\" width = 700 height = 250 alt=error></img>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1_add_img_n_add_mask = mask_add(img_n_add_mask, img_1_mask, (255, 0, 255))\n",
    "\n",
    "plt.imshow(img_1_add_img_n_add_mask)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 결과물 (예시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OC_output = cv2.imread('example/OC_output.png', cv2.IMREAD_COLOR)\n",
    "# SB_output = cv2.imread('example/SB_output.png', cv2.IMREAD_COLOR)\n",
    "# SH_output = cv2.imread('example/SH_output.png', cv2.IMREAD_COLOR)\n",
    "# UD_output = cv2.imread('example/UD_output.png', cv2.IMREAD_COLOR)\n",
    "# YA_output = cv2.imread('example/YA_output.png', cv2.IMREAD_COLOR)\n",
    "\n",
    "# # 이미지를 BGR에서 RGB로 변환\n",
    "# OC_output = cv2.cvtColor(OC_output, cv2.COLOR_BGR2RGB)\n",
    "# SB_output = cv2.cvtColor(SB_output, cv2.COLOR_BGR2RGB)\n",
    "# SH_output = cv2.cvtColor(SH_output, cv2.COLOR_BGR2RGB)\n",
    "# UD_output = cv2.cvtColor(UD_output, cv2.COLOR_BGR2RGB)\n",
    "# YA_output = cv2.cvtColor(YA_output, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# fig, axes = plt.subplots(5, 1, figsize=(100, 25))\n",
    "\n",
    "# axes[0].imshow(OC_output)\n",
    "# axes[0].axis('off')\n",
    "# axes[0].set_title('OC_output: level 2')\n",
    "\n",
    "# axes[1].imshow(SB_output)\n",
    "# axes[1].axis('off')\n",
    "# axes[1].set_title('SB_output: level 2')\n",
    "\n",
    "# axes[2].imshow(SH_output)\n",
    "# axes[2].axis('off')\n",
    "# axes[2].set_title('SH_output: level 3')\n",
    "\n",
    "# axes[3].imshow(UD_output)\n",
    "# axes[3].axis('off')\n",
    "# axes[3].set_title('UD_output: level 5')\n",
    "\n",
    "\n",
    "# axes[4].imshow(YA_output)\n",
    "# axes[4].axis('off')\n",
    "# axes[4].set_title('YA_output: level 5')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_chim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
